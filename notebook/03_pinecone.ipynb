{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73db0a5",
   "metadata": {},
   "source": [
    "# PINECONEを用いたvectorサーチのデモ\n",
    "\n",
    "1. 技術ブログの検索\n",
    "  - 決まったフォーマットで出力\n",
    "  - URLも同時に返却\n",
    "  - 上位3つくらい\n",
    "\n",
    "2. 技術ブログの登録\n",
    "\n",
    "- https://gpt-index.readthedocs.io/en/v0.5.27/how_to/integrations/vector_stores.html\n",
    "\n",
    "\n",
    "# 方針\n",
    "\n",
    "- LangChain.Agentを利用せずに、LLamaIndex単体で実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ecb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import SimpleWebPageReader, LLMPredictor, ServiceContext, GPTPineconeIndex, OpenAIEmbedding\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "import pinecone\n",
    "from langchain import OpenAI\n",
    "\n",
    "INDEX_NAME = \"chatgpt-search-index\"\n",
    "\n",
    "PREDICTOR_MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "INITIAL_URLS = [\n",
    "    \"https://dev.classmethod.jp/articles/lang-chain-agent-customized-by-llama-index-tool/\",\n",
    "    \"https://yukoishizaki.hatenablog.com/entry/2020/05/24/145155\",\n",
    "    \"https://runble1.com/gcp-terraform-cloud-run/\"\n",
    "]\n",
    "ADDITIONAL_URL = \"https://zenn.dev/tfutada/articles/acf8adbb2ba5be\"\n",
    "\n",
    "# カスタムテンプレートの作成\n",
    "CUSTOM_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"コンテキストは以下です. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"コンテキストが与えられた場合, \"\n",
    "    \"質問に回答してください: {query_str}\\n\"\n",
    ")\n",
    "CUSTOM_TEXT_QA_PROMPT = QuestionAnswerPrompt(CUSTOM_TEXT_QA_PROMPT_TMPL)\n",
    "\n",
    "CUSTOM_REFINE_PROMPT_TMPL = (\n",
    "    \"元の質問: {query_str}\\n\"\n",
    "    \"オリジナルの回答: {existing_answer}\\n\"\n",
    "    \"以下のコンテキストを使って、オリジナルの回答を推敲することができます.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"コンテキストを元に、オリジナルの回答を、より元の質問に沿ったものに推敲してください. \"\n",
    "    \"もしコンテキストが有用なものでなければ、オリジナルの回答を返却してください\"\n",
    ")\n",
    "CUSTOM_REFINE_PROMPT = RefinePrompt(CUSTOM_REFINE_PROMPT_TMPL)\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバッグする際に実行\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d8bc1",
   "metadata": {},
   "source": [
    "# Indexの構築 & 検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc98b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientの作成\n",
    "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENVIRONMENT\"])\n",
    "pinecone.create_index(\n",
    "    INDEX_NAME,\n",
    "    dimension=1536, \n",
    "    metric=\"euclidean\", \n",
    "    pod_type=\"p1\"\n",
    ")\n",
    "pinecone_index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889988e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e877a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5422f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webページからdocumentクラスを作成\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(INITIAL_URLS)\n",
    "\n",
    "# extra_infoにurlを追加\n",
    "for document, url in zip(documents, INITIAL_URLS):\n",
    "    document.extra_info = {\"url\": url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d4c4d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:158: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:661: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "WARNING:llama_index.vector_stores.pinecone:If directly passing in client, cannot automatically reconstruct connetion after save_to_disk/load_from_disk.For automatic reload, store PINECONE_API_KEY in env variable and pass in index_name and environment instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c189f306a0b943b8af9ae274aaf19cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138376fa346f43b9880926d2c6babbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70f52d617b842d4ab809cfe172348ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e18c047f4064eba86dc26f23632c175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 29979 tokens\n"
     ]
    }
   ],
   "source": [
    "# 既にDBにdocumentが存在する場合、その内容を利用\n",
    "if pinecone_index.describe_index_stats().total_vector_count > 0:\n",
    "    initial_documents = []\n",
    "else:\n",
    "    initial_documents = documents\n",
    "\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=OpenAI(\n",
    "        temperature=0, model_name=PREDICTOR_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=EMBEDDING_MODEL_NAME\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "index = GPTPineconeIndex.from_documents(\n",
    "    pinecone_index=pinecone_index,\n",
    "    documents=initial_documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c251efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 17252 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Calibrationは、測定器やモデルの出力値を正確に調整するための技術であり、信頼性を高めることを指します。機械学習においては、モデルの予測値と実際の値の分布が異なる場合に、モデルの信頼性を高めるために必要です。具体的には、確率予測の信頼度を高めるために、Sigmoid/Platt ScaleやIsotonic Regressionなどの手法があります。Isotonic Regressionは、単調非減少な回帰曲線を学習し、確率予測の信頼性を高める手法です。また、Calibration Curveを使って確率予測の信頼度を可視化することもできます。Calibrationの評価指標としては、Brier Scoreがよく使われます。Brier Scoreは、確率予測した値とラベル(0,1)との二乗誤差の平均を表します。ただし、LightGBMや最近のNNは自信過剰で、Calibrationが不十分であるという報告もあります。\n",
      "参照url1: https://yukoishizaki.hatenablog.com/entry/2020/05/24/145155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/data_structs/node_v2.py:181: UserWarning: .extra_info is deprecated, use .node.extra_info instead\n",
      "  warnings.warn(\".extra_info is deprecated, use .node.extra_info instead\")\n"
     ]
    }
   ],
   "source": [
    "search_query = \"calibrationってどんな技術だっけ？\"\n",
    "\n",
    "response = index.query(\n",
    "    search_query,\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=CUSTOM_TEXT_QA_PROMPT,\n",
    "    refine_template=CUSTOM_REFINE_PROMPT\n",
    ")\n",
    "\n",
    "print(f\"response: {response.response}\")\n",
    "\n",
    "reffer_urls = set([source_node.extra_info[\"url\"] for source_node in response.source_nodes])\n",
    "for i, url in enumerate(reffer_urls):\n",
    "    print(f\"参照url{i+1}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5241d",
   "metadata": {},
   "source": [
    "# Documentの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ea08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10}},\n",
       " 'total_vector_count': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7012dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dd154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75090193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 13970 tokens\n"
     ]
    }
   ],
   "source": [
    "query_response = pinecone_index.query(\n",
    "    top_k=10,\n",
    "    include_metadata=True,\n",
    "    # dummyのベクトル\n",
    "    vector=[0.1] * 1536,\n",
    "    filter={\n",
    "         \"extra_info_url\": {\"$in\": [ADDITIONAL_URL]}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 既に登録されているURLの場合はskip\n",
    "if len(query_response[\"matches\"]) == 0:\n",
    "    \n",
    "    additional_urls = [ADDITIONAL_URL]\n",
    "    additional_documents = SimpleWebPageReader(html_to_text=True).load_data(additional_urls)\n",
    "\n",
    "    # extra_infoにurlを追加\n",
    "    for additional_document, url in zip(additional_documents, additional_urls):\n",
    "        additional_document.extra_info = {\"url\": url}\n",
    "    \n",
    "    # insert\n",
    "    for additional_document in additional_documents:\n",
    "        index.insert(additional_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "530938e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 14}},\n",
       " 'total_vector_count': 14}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8176638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 15872 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Qdrantの特徴は、オープンソースのRust製ベクトル検索エンジンであり、Python SDK、REST API、gRPCで接続できること、クラウドサービス版も準備中であること、ベクトル検索においてコサイン類似度を使用していること、他のベクトル検索エンジンと比較して高速であることなどが挙げられます。Qdrantは、機械学習関連のブログでよく使用されるlivedoorニュースコーパスなどのベクトルデータを取り込むことができます。また、Qdrantの他にも、Facebook FaissやPynndescentなどのライブラリもベクトル検索に使用できます。Qdrantは、コレクションとポイントの概念を持ち、コレクションはRDBのテーブルに、ポイントはRDBのレコードに相当します。Qdrantでは、ペイロードと呼ばれるメタ情報も一緒に登録でき、フィルター検索に使用できます。Python SDKを使用することで、コレクションの作成、ドキュメントの登録、類似ドキュメントの検索が可能です。また、Qdrantはベクトル検索した後でフィルタリングすることができるため、検索結果が少なくなることを防ぐことができます。さらに、Qdrantはバッチクエリーという機能を持ち、一度に複数のクエリーを投げることができます。また、レコメンドAPIを使用することで、インデックスされたポイントを使用して検索できます。\n",
      "参照url1: https://zenn.dev/tfutada/articles/acf8adbb2ba5be\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Qdrantの特徴は?\"\n",
    "\n",
    "response = index.query(\n",
    "    search_query,\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=CUSTOM_TEXT_QA_PROMPT,\n",
    "    refine_template=CUSTOM_REFINE_PROMPT\n",
    ")\n",
    "\n",
    "print(f\"response: {response.response}\")\n",
    "\n",
    "reffer_urls = set([source_node.extra_info[\"url\"] for source_node in response.source_nodes])\n",
    "for i, url in enumerate(reffer_urls):\n",
    "    print(f\"参照url{i+1}: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2af71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3466fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexをローカルに保存\n",
    "index.save_to_disk(\"../data/pinecone_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595663eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838ed3b0",
   "metadata": {},
   "source": [
    "# index insertエラー時"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86ea369",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENVIRONMENT\"])\n",
    "pinecone_index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b4291c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'openai_api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m LLMPredictor(\n\u001b[1;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOpenAI(\n\u001b[1;32m      3\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39mPREDICTOR_MODEL_NAME, openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m     12\u001b[0m     llm_predictor\u001b[38;5;241m=\u001b[39mllm_predictor,\n\u001b[1;32m     13\u001b[0m     embed_model\u001b[38;5;241m=\u001b[39membed_model\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m index \u001b[38;5;241m=\u001b[39m GPTPineconeIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m     17\u001b[0m     pinecone_index\u001b[38;5;241m=\u001b[39mpinecone_index,\n\u001b[1;32m     18\u001b[0m     documents\u001b[38;5;241m=\u001b[39m[], service_context\u001b[38;5;241m=\u001b[39mservice_context\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/embeddings/openai.py:209\u001b[0m, in \u001b[0;36mOpenAIEmbedding.__init__\u001b[0;34m(self, mode, model, deployment_name, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m OpenAIEmbeddingMode\u001b[38;5;241m.\u001b[39mTEXT_SEARCH_MODE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m OpenAIEmbeddingMode(mode)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m OpenAIEmbeddingModelType(model)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'openai_api_key'"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(\n",
    "    llm=OpenAI(\n",
    "        temperature=0, model_name=PREDICTOR_MODEL_NAME, openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=EMBEDDING_MODEL_NAME, openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "index = GPTPineconeIndex.from_documents(\n",
    "    pinecone_index=pinecone_index,\n",
    "    documents=[], service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "543516ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m additional_document, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(additional_documents, additional_urls):\n\u001b[1;32m      6\u001b[0m     additional_document\u001b[38;5;241m.\u001b[39mextra_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url}\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43madditional_document\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/indices/base.py:148\u001b[0m, in \u001b[0;36mBaseGPTIndex.insert\u001b[0;34m(self, document, **insert_kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: Document, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Insert a document.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/node_parser/simple.py:47\u001b[0m, in \u001b[0;36mSimpleNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     45\u001b[0m all_nodes: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 47\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mget_nodes_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_include_extra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_prev_next_rel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_include_prev_next_rel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     all_nodes\u001b[38;5;241m.\u001b[39mextend(nodes)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/node_parser/node_utils.py:49\u001b[0m, in \u001b[0;36mget_nodes_from_document\u001b[0;34m(document, text_splitter, include_extra_info, include_prev_next_rel)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nodes_from_document\u001b[39m(\n\u001b[1;32m     43\u001b[0m     document: BaseDocument,\n\u001b[1;32m     44\u001b[0m     text_splitter: TextSplitter,\n\u001b[1;32m     45\u001b[0m     include_extra_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m     include_prev_next_rel: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Node]:\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get nodes from document.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     text_splits \u001b[38;5;241m=\u001b[39m \u001b[43mget_text_splits_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_extra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     nodes: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m     index_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/node_parser/node_utils.py:29\u001b[0m, in \u001b[0;36mget_text_splits_from_document\u001b[0;34m(document, text_splitter, include_extra_info)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# TODO: clean up since this only exists due to the diff w LangChain's TextSplitter\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_splitter, TokenTextSplitter):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# use this to extract extra information about the chunks\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     text_splits \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text_with_overlaps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_info_str\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minclude_extra_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     text_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(\n\u001b[1;32m     35\u001b[0m         document\u001b[38;5;241m.\u001b[39mget_text(),\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/langchain_helpers/text_splitter.py:144\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text_with_overlaps\u001b[0;34m(self, text, extra_info_str)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# First we naively split the large input into a bunch of smaller ones.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m splits \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator)\n\u001b[0;32m--> 144\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# We now want to combine these smaller pieces into medium size\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# chunks to send to the LLM.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m docs: List[TextSplit] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/langchain_helpers/text_splitter.py:78\u001b[0m, in \u001b[0;36mTokenTextSplitter._preprocess_splits\u001b[0;34m(self, splits, chunk_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m new_splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[0;32m---> 78\u001b[0m     num_cur_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_cur_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_size:\n\u001b[1;32m     80\u001b[0m         new_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/tiktoken/core.py:117\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[38;5;241m.\u001b[39msearch(text):\n\u001b[0;32m--> 117\u001b[0m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_core_bpe\u001b[38;5;241m.\u001b[39mencode(text, allowed_special)\n",
      "File \u001b[0;32m~/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/tiktoken/core.py:337\u001b[0m, in \u001b[0;36mraise_disallowed_special_token\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as a special token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "additional_urls = [\"https://note.com/npaka/n/n2e2cf5a458ac\"]\n",
    "additional_documents = SimpleWebPageReader(html_to_text=True).load_data(additional_urls)\n",
    "\n",
    "# extra_infoにurlを追加\n",
    "for additional_document, url in zip(additional_documents, additional_urls):\n",
    "    additional_document.extra_info = {\"url\": url}\n",
    "    index.insert(additional_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "535c476a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAIEmbeddingModelType' from 'llama_index' (/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddingModelType\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAIEmbeddingModelType' from 'llama_index' (/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/__init__.py)"
     ]
    }
   ],
   "source": [
    "from llama_index import OpenAIEmbeddingModelType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6361606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
