{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73db0a5",
   "metadata": {},
   "source": [
    "# PINECONEを用いたvectorサーチのデモ\n",
    "\n",
    "1. 技術ブログの検索\n",
    "  - 決まったフォーマットで出力\n",
    "  - URLも同時に返却\n",
    "  - 上位3つくらい\n",
    "\n",
    "2. 技術ブログの登録\n",
    "\n",
    "- https://gpt-index.readthedocs.io/en/v0.5.27/how_to/integrations/vector_stores.html\n",
    "\n",
    "\n",
    "# 方針\n",
    "\n",
    "- LangChain.Agentを利用せずに、LLamaIndex単体で実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ecb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import SimpleWebPageReader, LLMPredictor, ServiceContext, GPTPineconeIndex, OpenAIEmbedding\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "import pinecone\n",
    "from langchain import OpenAI\n",
    "\n",
    "INDEX_NAME = \"chatgpt-search-index\"\n",
    "\n",
    "PREDICTOR_MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "INITIAL_URLS = [\n",
    "    \"https://dev.classmethod.jp/articles/lang-chain-agent-customized-by-llama-index-tool/\",\n",
    "    \"https://yukoishizaki.hatenablog.com/entry/2020/05/24/145155\",\n",
    "    \"https://runble1.com/gcp-terraform-cloud-run/\"\n",
    "]\n",
    "ADDITIONAL_URL = \"https://zenn.dev/tfutada/articles/acf8adbb2ba5be\"\n",
    "\n",
    "# カスタムテンプレートの作成\n",
    "CUSTOM_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"コンテキストは以下です. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"コンテキストが与えられた場合, \"\n",
    "    \"質問に回答してください: {query_str}\\n\"\n",
    ")\n",
    "CUSTOM_TEXT_QA_PROMPT = QuestionAnswerPrompt(CUSTOM_TEXT_QA_PROMPT_TMPL)\n",
    "\n",
    "CUSTOM_REFINE_PROMPT_TMPL = (\n",
    "    \"元の質問: {query_str}\\n\"\n",
    "    \"オリジナルの回答: {existing_answer}\\n\"\n",
    "    \"以下のコンテキストを使って、オリジナルの回答を推敲することができます.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"コンテキストを元に、オリジナルの回答を、より元の質問に沿ったものに推敲してください. \"\n",
    "    \"もしコンテキストが有用なものでなければ、オリジナルの回答を返却してください\"\n",
    ")\n",
    "CUSTOM_REFINE_PROMPT = RefinePrompt(CUSTOM_REFINE_PROMPT_TMPL)\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバッグする際に実行\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d8bc1",
   "metadata": {},
   "source": [
    "# Indexの構築 & 検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc98b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientの作成\n",
    "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENVIRONMENT\"])\n",
    "pinecone.create_index(\n",
    "    INDEX_NAME,\n",
    "    dimension=1536, \n",
    "    metric=\"euclidean\", \n",
    "    pod_type=\"p1\"\n",
    ")\n",
    "pinecone_index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889988e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e877a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5422f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webページからdocumentクラスを作成\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(INITIAL_URLS)\n",
    "\n",
    "# extra_infoにurlを追加\n",
    "for document, url in zip(documents, INITIAL_URLS):\n",
    "    document.extra_info = {\"url\": url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d4c4d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:158: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/langchain/llms/openai.py:661: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "WARNING:llama_index.vector_stores.pinecone:If directly passing in client, cannot automatically reconstruct connetion after save_to_disk/load_from_disk.For automatic reload, store PINECONE_API_KEY in env variable and pass in index_name and environment instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c189f306a0b943b8af9ae274aaf19cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138376fa346f43b9880926d2c6babbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70f52d617b842d4ab809cfe172348ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e18c047f4064eba86dc26f23632c175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 29979 tokens\n"
     ]
    }
   ],
   "source": [
    "# 既にDBにdocumentが存在する場合、その内容を利用\n",
    "if pinecone_index.describe_index_stats().total_vector_count > 0:\n",
    "    initial_documents = []\n",
    "else:\n",
    "    initial_documents = documents\n",
    "\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=OpenAI(\n",
    "        temperature=0, model_name=PREDICTOR_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=EMBEDDING_MODEL_NAME\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "index = GPTPineconeIndex.from_documents(\n",
    "    pinecone_index=pinecone_index,\n",
    "    documents=initial_documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c251efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 17252 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Calibrationは、測定器やモデルの出力値を正確に調整するための技術であり、信頼性を高めることを指します。機械学習においては、モデルの予測値と実際の値の分布が異なる場合に、モデルの信頼性を高めるために必要です。具体的には、確率予測の信頼度を高めるために、Sigmoid/Platt ScaleやIsotonic Regressionなどの手法があります。Isotonic Regressionは、単調非減少な回帰曲線を学習し、確率予測の信頼性を高める手法です。また、Calibration Curveを使って確率予測の信頼度を可視化することもできます。Calibrationの評価指標としては、Brier Scoreがよく使われます。Brier Scoreは、確率予測した値とラベル(0,1)との二乗誤差の平均を表します。ただし、LightGBMや最近のNNは自信過剰で、Calibrationが不十分であるという報告もあります。\n",
      "参照url1: https://yukoishizaki.hatenablog.com/entry/2020/05/24/145155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikegamikenshin/Documents/study/chatgpt-practice/.venv/lib/python3.9/site-packages/llama_index/data_structs/node_v2.py:181: UserWarning: .extra_info is deprecated, use .node.extra_info instead\n",
      "  warnings.warn(\".extra_info is deprecated, use .node.extra_info instead\")\n"
     ]
    }
   ],
   "source": [
    "search_query = \"calibrationってどんな技術だっけ？\"\n",
    "\n",
    "response = index.query(\n",
    "    search_query,\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=CUSTOM_TEXT_QA_PROMPT,\n",
    "    refine_template=CUSTOM_REFINE_PROMPT\n",
    ")\n",
    "\n",
    "print(f\"response: {response.response}\")\n",
    "\n",
    "reffer_urls = set([source_node.extra_info[\"url\"] for source_node in response.source_nodes])\n",
    "for i, url in enumerate(reffer_urls):\n",
    "    print(f\"参照url{i+1}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5241d",
   "metadata": {},
   "source": [
    "# Documentの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ea08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10}},\n",
       " 'total_vector_count': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7012dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dd154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75090193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 13970 tokens\n"
     ]
    }
   ],
   "source": [
    "query_response = pinecone_index.query(\n",
    "    top_k=10,\n",
    "    include_metadata=True,\n",
    "    # dummyのベクトル\n",
    "    vector=[0.1] * 1536,\n",
    "    filter={\n",
    "         \"extra_info_url\": {\"$in\": [ADDITIONAL_URL]}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 既に登録されているURLの場合はskip\n",
    "if len(query_response[\"matches\"]) == 0:\n",
    "    \n",
    "    additional_urls = [ADDITIONAL_URL]\n",
    "    additional_documents = SimpleWebPageReader(html_to_text=True).load_data(additional_urls)\n",
    "\n",
    "    # extra_infoにurlを追加\n",
    "    for additional_document, url in zip(additional_documents, additional_urls):\n",
    "        additional_document.extra_info = {\"url\": url}\n",
    "    \n",
    "    # insert\n",
    "    for additional_document in additional_documents:\n",
    "        index.insert(additional_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "530938e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 14}},\n",
       " 'total_vector_count': 14}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8176638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 15872 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Qdrantの特徴は、オープンソースのRust製ベクトル検索エンジンであり、Python SDK、REST API、gRPCで接続できること、クラウドサービス版も準備中であること、ベクトル検索においてコサイン類似度を使用していること、他のベクトル検索エンジンと比較して高速であることなどが挙げられます。Qdrantは、機械学習関連のブログでよく使用されるlivedoorニュースコーパスなどのベクトルデータを取り込むことができます。また、Qdrantの他にも、Facebook FaissやPynndescentなどのライブラリもベクトル検索に使用できます。Qdrantは、コレクションとポイントの概念を持ち、コレクションはRDBのテーブルに、ポイントはRDBのレコードに相当します。Qdrantでは、ペイロードと呼ばれるメタ情報も一緒に登録でき、フィルター検索に使用できます。Python SDKを使用することで、コレクションの作成、ドキュメントの登録、類似ドキュメントの検索が可能です。また、Qdrantはベクトル検索した後でフィルタリングすることができるため、検索結果が少なくなることを防ぐことができます。さらに、Qdrantはバッチクエリーという機能を持ち、一度に複数のクエリーを投げることができます。また、レコメンドAPIを使用することで、インデックスされたポイントを使用して検索できます。\n",
      "参照url1: https://zenn.dev/tfutada/articles/acf8adbb2ba5be\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Qdrantの特徴は?\"\n",
    "\n",
    "response = index.query(\n",
    "    search_query,\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=CUSTOM_TEXT_QA_PROMPT,\n",
    "    refine_template=CUSTOM_REFINE_PROMPT\n",
    ")\n",
    "\n",
    "print(f\"response: {response.response}\")\n",
    "\n",
    "reffer_urls = set([source_node.extra_info[\"url\"] for source_node in response.source_nodes])\n",
    "for i, url in enumerate(reffer_urls):\n",
    "    print(f\"参照url{i+1}: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2af71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3466fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexをローカルに保存\n",
    "index.save_to_disk(\"../data/pinecone_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595663eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1fcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
